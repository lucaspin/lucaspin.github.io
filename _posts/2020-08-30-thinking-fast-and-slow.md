---
layout: post
title: "Thinking, Fast and Slow"
categories: [Book, Psychology, Economics]
---

> Our reconforting conviction that the world makes sense lies on a secure foundation: our near unlimited capacity of ignoring our own ignorance.

This book offers a comprehensive insight into the shortcomings of our own minds, and it does that not only by defining concepts and referencing articles, but also by tricking you as you read it through as an example of the biases and heuristics it exposes.

The author basically builds up a catalog of different biases, defining them by name and describing their "symptoms" (he compares that to the way we associate a disease to its symptoms).

Basically, all the cognitive and mental biases derive from the fact that mobilizing our brain to concentrate and focus is expensive, and therefore, when confronted with a hard question, we respond to an easier one instead, without even noticing the substitution we just made.

# Focus

Focusing intensely on a task can effectively blind us, even to stimulus that generally draw our attention. Also, we don't become just blind to the obvious, but when that happens, we usually also neglect our own blindness.
*gorilla study*

He compares our mind to the electrical circuits in our houses. Both have a limited capacity, but react differently under stress. In an electrical circuit, a circuit breaker opens when the current is excessive, which leads to all the devices in that circuit to lose its power. Our mind, however, is more selective and, under stress, protects the most important activity, shifting all the available attention to it. I'd compare that to a cpu instead of an electrical circuit, but I see the merit in the analogy.

That way, intelligence isn't just related to cognitive ability, but also related to the capacity to find relevant information in our brains and mobilize attention when necessary. Also, as we become more proficient at a task, the amount of energy required to perform that task decreases; brain studies reveal less brain activity and less brain areas involved.

# Normality

> How many animals of each kind did Moses take on the ark?

I fell hard on that question, and knowing that [I wasn't alone](https://www.oxfordreference.com/view/10.1093/oi/authority.20110803100211686) wasn't as reassuring as I'd like.

We maintain and update a model of our personal world, representing what's normal in it, making surprise the most sensible indicative of how we comprehend our world and we expect from it. Brain reaction studies show that normality violations are detected really fast and subtly.

*Animals entering an ark* bring up the biblical context, in which Moses isn't abnormal, and because of that our *normality bell* doesn't ring and we don't spot the mistake in the question.

# We can be dominated by our feelings

We are not very good in separating our feelings from our cognitive conclusions.

## Halo Effect

We have the tendency of liking/disliking everything about a person (even the things we didn't observe yet), and that is related to the sequence in which we observe the characteristics of someone.

The first impressions have higher weights than the subsequent ones; sometimes the later observed ones are completely ignored.

That sometimes leads us to look at causual relations in reverse; *company is failing because the CEO is inflexible* instead of *the CEO is inflexible because the company is failing*.

## Affect Heuristic

> instead of responding to the question *What do I think about this?*, we answer to the question *How do I feel about this?*.

Proposed by [Paul Slovic](https://en.wikipedia.org/wiki/Paul_Slovic). We let our emotional attitude about a topic (sympathies and antipathies) determine our beliefs, and because of that we tend to overestimate the benefits/advantages and understimate the costs/disadvantages of things we like or believe in.

# We are deeply influenced by what we see (and ignore what we don't)

## Availability Heuristic

We judge a specific topic/decision/concept by the first examples of ocurrences of it (and how easy they are retrieved from our memory) that are provided to us by our brains; and how that may be related to the media content we consume.

We substitute the size of a category or the frequency of an event for the impression of how easy it is to recall informations about it. That way, we overestimate the frequency of events that capture our attention more easily (and therefore are easily retrieved).

*A plane crash that is highly covered by the media makes you trust less in the safety of airplanes*.

Also, personal experiences are more easily recalled than situations that happened with other people.

*Couples usually overestimate (think they did more than their partner) their contributions to keep the house in order*
*group member feel they did more than their fair share of work*

That is, we occasionaly do more than our fair share, but it's good to know we feel that way regardless of how much work we actually do.

That also interferes with the way we view ourselves: if we remember easily of occasions in which we were kind, we conclude that we are kind; and that assertion is related to how easy it is to recall, not the absolule amount of recollections (if we recollect more occasions, but take longer to do so, we tend to conclude we are not that kind).

## WYSIATI

*what you see is all there is*

We form conclusions by only looking at the evidence we see at the moment, completely ignoring that we may have critical information missing, or even that the evidence we have is not trustworthy.

Always good to remember that the best way to effectively extract information from multiple sources is making these sources independent from one another.

## Priming

> *EAT -> SO_P -> SOUP* and not *EAT -> SO_P -> SOAP*

It is weird to acknowledge the fact that our actions and emotions can be influenced by simple things, like exposure to words or sounds. The simple exposure to a word causes immediate and measurable changes on how easily a lot of related words are recalled.

That happens because of *associative activation*: ideas that were evoked trigger other related ideas, in a growing cascade of activity in our brains. It goes beyond that: the [ideomotor phenomenon](https://en.wikipedia.org/wiki/Ideomotor_phenomenon) describes how an idea can trigger an unconscious action.

## Anchoring Effect

it happens when we consider a particular value for an unknown quantity before estimating it; our estimation will likely be near the considered value.

## Representativeness Heuristic

> The amount of concern is not appropriate to the probability of damage.

One characteristic of that heuristic is the excessive predisposition to predict the ocorrence of improbable events. Another one is insensitivity to the quality of the evidence.

## Hindsight Bias

> I KNEW IT!

No, you didn't know. Our inability to reconstruct past beliefs leads us to underestimate the measurement in which we were surprised by past events. And that makes we believe you knew it all along. But let's be honest: you didn't really know, specially if you consider that intuitive predictions tend to be overconfident and excessively extreme.

This is also related to the [Outcome Bias](https://en.wikipedia.org/wiki/Outcome_bias), in which we usually blame people for making good decisions that turned out badly and give them little credit for successful decisions that are only obvious after they happen.

# We are really bad at understating statistics

## Base Rate Fallacy

We tend to underestime or even ignore base rates, when some specific information about a case is available. We are also reluctant to deduce the particular from the general and we are inclined to infer the general from the particular.

## Mean Regression

events that are bad are usually followed by events that are better, and events that are good are usually followed by events that are worse.

discovered and named in the XIX century by sir Francis Galton in __Regression towards Mediocrity in Hereditary Stature__.

he demonstrated using (?) seeds that if the parents were bigger, the seeds were usually smaller; and if the parent smaller, the seeds were usually bigger.

Later on, he concluded that the mean regression inevitably happens when the correlation between two measurements isn't perfect

*complement/critique example*

## Correlation coefficient

The correlation coefficient between two measurements, that varies between 0 and 1, is a measurement of relative weight of the shared factors between them

*A correlação entre o tamanho de objetos medidos com precisão em inglês ou em unidades métricas é 1. Qualquer fator que influencie uma medição também influencia a outra; 100% dos determinantes são compartilhados*

## Conjuntion Fallacy

> Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1) Linda is a bank teller or 2) Linda is a bank teller and is active in the feminist movement.

When you specify an event in more detail, you can only reduce its probability. Surprisingly, [people tend to choose #2](https://en.wikipedia.org/wiki/Conjunction_fallacy). We sometimes classifies a conjunction of events as being more likely than one of the events.

# We also can be really bad at planning

## Planning fallacy

> An impartial appreciation of the uncertainty is the foundation of rationality.

When we have information about an individual case, we rarely feel the need to know statistics about the class to which the case belongs to (even if we know those statistics). The [Planning fallacy](https://en.wikipedia.org/wiki/Planning_fallacy) describes how we can be too optimistic to estimate the time needed for task.

How do we battle that? Using [Reference Class Forsecasting](https://en.wikipedia.org/wiki/Reference_class_forecasting). The author also mentions a method called [Pre-Mortem](https://en.wikipedia.org/wiki/Pre-mortem), by [Gary Klein](https://en.wikipedia.org/wiki/Gary_A._Klein) where the team imagines a scenario where the project failed, and tries to come up backwards with the reasons that could potentially have lead to the failure. 

> Doubt supression contributes to overconfidence.

# We don't like to lose things

## endowment effect

We tend to value the goods we keep "to use" more than the goods we keep "to trade". Selling goods that we'd use usually activates the areas of the brains associated with disgust and pain.

# We trust too much on intution

> Intution is nothing more, nothing more than recognition. You can't trust intuition on the absence of stable regularity in the environment

It talks about [Paul Meehl](https://en.wikipedia.org/wiki/Paul_E._Meehl)'s research on *clinical vs statistical prediction*. Basically, if previsibility is weak, inconsistency is destructive for any prognostic validity.

The key factors to trust an intuitive judgment are:
1. an environment regular enough to be predictive
2. the opportunity to learn those regularities through extended practice; and by using a quick and quality feedback loop.

It also mentions environments that are worse than just irregular, and might make the professionals learn the wrong lessons from it. Allegations of correct intuitions in an unpredictable environment are an illusion at best.

# we evaluate gains and loss asymmetrically

That is the base of his [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory). When directly compared ou pondered, losses are taken as larger than gains, and that is evolutionary: *organisms that treat threats as more urgent than opportunities have a better chance of surviving*. That's [loss aversion](https://en.wikipedia.org/wiki/Loss_aversion).

*that's why settlements are so complicated: concessions that you make for me are my gains, but are also your losses, and you assign to them a higher value than I do*

It offers an improvement in [Expected Utility Hipothesis](https://en.wikipedia.org/wiki/Expected_utility_hypothesis), by taking into account a referential point; people's current and specific situation might influence their decisions.

**fourfold pattern**

## possibility and certainty effects

> the weights attributed to results are not identical to the probability of these results.

*0% -> 5%*
Highly unlikely results receive a higher weight than they deserve. The lottery ticket is the supreme example of the possibility effect.

*95% -> 100%*
Highly certain results receive less weight than they should. We tend to exaggerate the weight of little risks and pay a higher value than expected to eliminate it completely. That's the basic principle that makes insurance policies so lucrative.

# We react differently to the same problem depending on how it is framed

## frequency format hipothesis

> 1,286 in 10,000 seems more likely than 12,86%.

That's why a good lawyer that wishes to raise doubt in a jury won't say that *the chance of a false positive is 0,1%*. 1 in a 10,000 seems more likely.

# We are not very good at investing either

The [Disposition Effect](https://en.wikipedia.org/wiki/Disposition_effect) is a tendency to sell assets that have increased in value, while keeping assets that have dropped in value. The [Sunk Cost Fallacy](https://en.wikipedia.org/wiki/Sunk_cost) is when we keep investing on something (that may not be that good) as a result of a previous investment. That may explain why people remain on terrible jobs or marriages.

# regret and normality

Regret is a counter-factual emotion that is evoked by the availability of reality alternatives. An abnormal event draws attention, and also evokes the idea of the event that would've been normal under the same circunstances.

> Mr. Brown never gives rides. Yesterday, he gave a ride to a guy and was mugged. Mr. Smith always gives rides. Yesterday, he gave a ride to a guy and was mugged. Who feels the most regret? Mr. Brown or Mr. Smith?

We also expect to have stronger emotional reactions (including regret) facing a result generated by a direct action.
Everything relates to the default option: keeping a stock is the default option in the stock market, so selling it is a deviation, and therefore, a natural candidate to regret.

The intense aversion to trading a risk for some other advantage is even a big part of our legal world, e.g. [the precautionary principle](https://en.wikipedia.org/wiki/Precautionary_principle).

## framing

Different formulations can result in different results.
*countries where organ donation is the default option have way more donors than countries where the default option is not being a donor*

## We don't remember things right

experiencing self -> is it hurting now?
remembering self -> how was it overall?

We confuse our past experiences with the memory we have of them. What we learn from the past is how to maximize the quality of our future memories, not necessarily our futures experiences.

*cold-hand-experiment: https://mindhacks.com/2013/05/28/why-you-might-prefer-more-pain/*

Our memories are highly influenced by two rules:
- __peak-end rule__: the global retrospective classification is the mean of the worst moment of the experience and on its end.
- __duration neglect__: the duration of an episode has no effect on its evaluation.

There's an interesting inconsistency here: we want our pleasures to be sustained and our pains to be brief, but our memory evoluted to represent the most intense moment and the end of an pleasure/pain episode.

## focusing illusion

> Nothing in life is as important as when you're thinking about it.

Adapting to a new situation, good or bad, consists in thinking less and less about it. That illusion creates a bias in favor of goods/experiences that are initially exciting, even if they lose their appeal after some time. And as we also neglect the duration of experiences, we also tend to under-appreciate those that conserve their value in the long run.
