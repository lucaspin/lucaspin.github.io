---
layout: post
title: "Thinking, Fast and Slow"
categories: [Book, Psychology]
---

> Our recomforting conviction that the world makes sense lies on a secure foundation: our nearly unlimited capacity of ignoring our own ignorance.

We enjoy thinking of ourselves as highly rational beings, making perfect and well-balanced decisions based on nothing more than evidence and probability. Well, reading this book might show how flawed that perception is, while offering a comprehensive insight into the shortcomings of our minds.

Warning: [Daniel Kahneman](https://pt.wikipedia.org/wiki/Daniel_Kahneman) will attempt to trick you, constantly. And then he'll offer you an explanation of why you were just tricked. Thanks, I guess.

He builds a catalog of different behaviors and biases, gives them a name and describe their "symptoms".
Here are some of the common behaviors we so consistently bring to life.

## We can be dominated by our feelings

This was not a surprise to be quite honest. Even though we don't like to admit, we are emotional beings. Some might be more, some might be less, but one common denominator in us all is that we are not very good in separating our feelings from our cognitive conclusions.

### [Halo Effect](https://en.wikipedia.org/wiki/Halo_effect)

We have the tendency of liking/disliking everything about a person/company/brand; even the things we didn't observe yet.

Here, the sequence of the observed characterisctics is very important to our final conclusion, where the first impressions have higher weights than the subsequent ones (which are not rarely completely ignored).

That behavior can lead us to look at causual relations in reverse: *company is failing because the CEO is inflexible* instead of *the CEO is inflexible because the company is failing*.

### [Affect Heuristic](https://en.wikipedia.org/wiki/Affect_heuristic)

> instead of responding to the question **What do I think about this?**, we answer to the question **How do I feel about this?**.

Proposed by [Paul Slovic](https://en.wikipedia.org/wiki/Paul_Slovic). We let our emotional attitude ((sympathies and antipathies) about a topic determine our beliefs. That leads us to overestimate the benefits/advantages and understimate the costs/disadvantages of the things we like or believe in.

## We are deeply influenced by what we see

Give me a W, give me a Y, give me an S, give me an I, give me an A, give me a T, give me another I. What does that spell? WYSIATI! Daniel loves that acronym. Loves it. It appears everywhere in the book.

It is a concise way of saying that we form conclusions by only looking at the evidence we see at the moment, ignoring that we may have critical information missing, or even that the evidence we have is not trustworthy.

### [Availability Heuristic](https://en.wikipedia.org/wiki/Availability_heuristic)

We judge a specific topic/decision/concept by the first examples we can remember, and by how easy they are retrieved from our memory. We substitute the size of a category or the frequency of an event for the impression of how easy it is to recall information about it. That way, we overestimate the frequency of events that capture our attention more easily. Example: a plane crash that is highly covered by the media makes you trust less in the safety of airplanes.

Also, personal experiences are more easily recalled than other people's experiences: couples usually overestimate their contributions to keep the house in order and think they did more than their partner; a group member usually always feel they did more than their fair share of work.

We occasionaly do more than our fair share, but it's good to know we feel that way regardless of how much work we actually do.

That heuristic also interferes with the way we judge ourselves: if we easily remember of occasions in which we were kind, we conclude that we are kind. That assertion is related to how easy it is to recall, not the absolule amount of recollections (if we recollect more occasions, but take longer to do so, we tend to conclude we are not that kind).

### Anchoring Effect

it happens when we consider a particular value for an unknown quantity before estimating it; our estimation will likely be near the considered value.

### Representativeness Heuristic

> The amount of concern is not appropriate to the probability of damage.

One characteristic of that heuristic is the excessive predisposition to predict the ocorrence of improbable events. Another one is insensitivity to the quality of the evidence.

### Hindsight Bias

> I KNEW IT!

No, you didn't know. Our inability to reconstruct past beliefs leads us to underestimate the measurement in which we were surprised by past events. And that makes we believe you knew it all along. But let's be honest: you didn't really know, specially if you consider that intuitive predictions tend to be overconfident and excessively extreme.

This is also related to the [Outcome Bias](https://en.wikipedia.org/wiki/Outcome_bias), in which we usually blame people for making good decisions that turned out badly and give them little credit for successful decisions that are only obvious after they happen.

## We are influenced by the smallest things possible

### Priming

> *EAT -> SO_P -> SOUP* and not *EAT -> SO_P -> SOAP*

It is weird to acknowledge the fact that our actions and emotions can be influenced by simple things, like exposure to words or sounds. The simple exposure to a word causes immediate and measurable changes on how easily a lot of related words are recalled.

That happens because of *associative activation*: ideas that were evoked trigger other related ideas, in a growing cascade of activity in our brains. It goes beyond that: the [ideomotor phenomenon](https://en.wikipedia.org/wiki/Ideomotor_phenomenon) describes how an idea can trigger an unconscious action.

## We are really bad at understating statistics

### Base Rate Fallacy

We tend to underestime or even ignore base rates, when some specific information about a case is available. We are also reluctant to deduce the particular from the general and we are inclined to infer the general from the particular.

### Mean Regression

events that are bad are usually followed by events that are better, and events that are good are usually followed by events that are worse.

discovered and named in the XIX century by sir Francis Galton in __Regression towards Mediocrity in Hereditary Stature__.

he demonstrated using (?) seeds that if the parents were bigger, the seeds were usually smaller; and if the parent smaller, the seeds were usually bigger.

Later on, he concluded that the mean regression inevitably happens when the correlation between two measurements isn't perfect

*complement/critique example*

### Correlation coefficient

The correlation coefficient between two measurements, that varies between 0 and 1, is a measurement of relative weight of the shared factors between them

*A correlação entre o tamanho de objetos medidos com precisão em inglês ou em unidades métricas é 1. Qualquer fator que influencie uma medição também influencia a outra; 100% dos determinantes são compartilhados*

### Conjuntion Fallacy

> Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1) Linda is a bank teller or 2) Linda is a bank teller and is active in the feminist movement.

When you specify an event in more detail, you can only reduce its probability. Surprisingly, [people tend to choose #2](https://en.wikipedia.org/wiki/Conjunction_fallacy). We sometimes classifies a conjunction of events as being more likely than one of the events.

## We also can be really bad at planning

### Planning fallacy

> An impartial appreciation of the uncertainty is the foundation of rationality.

When we have information about an individual case, we rarely feel the need to know statistics about the class to which the case belongs to (even if we know those statistics). The [Planning fallacy](https://en.wikipedia.org/wiki/Planning_fallacy) describes how we can be too optimistic to estimate the time needed for task.

How do we battle that? Using [Reference Class Forsecasting](https://en.wikipedia.org/wiki/Reference_class_forecasting). The author also mentions a method called [Pre-Mortem](https://en.wikipedia.org/wiki/Pre-mortem), by [Gary Klein](https://en.wikipedia.org/wiki/Gary_A._Klein) where the team imagines a scenario where the project failed, and tries to come up backwards with the reasons that could potentially have lead to the failure. 

> Doubt supression contributes to overconfidence.

## We really don't like to lose things, and evaluate gains and losses asymmetrically

That is the base of his [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory). When directly compared ou pondered, losses are taken as larger than gains, and that is evolutionary: organisms that treat threats as more urgent than opportunities have a better chance of surviving. That's [loss aversion](https://en.wikipedia.org/wiki/Loss_aversion).

That's why settlements are so complicated: concessions that you make for me are my gains, but are also your losses, and you assign to them a higher value than I do.

It offers an improvement in [Expected Utility Hipothesis](https://en.wikipedia.org/wiki/Expected_utility_hypothesis), by taking into account a referential point; people's current and specific situation might influence their decisions.

**fourfold pattern**

## possibility and certainty effects

> the weights attributed to results are not identical to the probability of these results.

*0% -> 5%*
Highly unlikely results receive a higher weight than they deserve. The lottery ticket is the supreme example of the possibility effect.

*95% -> 100%*
Highly certain results receive less weight than they should. We tend to exaggerate the weight of little risks and pay a higher value than expected to eliminate it completely. That's the basic principle that makes insurance policies so lucrative.

### endowment effect

We tend to value the goods we keep "to use" more than the goods we keep "to trade". Selling goods that we'd use usually activates the areas of the brains associated with disgust and pain.

## We trust too much on intution

> Intution is nothing more, nothing more than recognition. You can't trust intuition on the absence of stable regularity in the environment

It talks about [Paul Meehl](https://en.wikipedia.org/wiki/Paul_E._Meehl)'s research on *clinical vs statistical prediction*. Basically, if previsibility is weak, inconsistency is destructive for any prognostic validity.

The key factors to trust an intuitive judgment are:
1. an environment regular enough to be predictive
2. the opportunity to learn those regularities through extended practice; and by using a quick and quality feedback loop.

It also mentions environments that are worse than just irregular, and might make the professionals learn the wrong lessons from it. Allegations of correct intuitions in an unpredictable environment are an illusion at best.

## We react differently to the same problem depending on how it is framed

Different formulations can result in different results.
*countries where organ donation is the default option have way more donors than countries where the default option is not being a donor*

### frequency format hipothesis

> 1,286 in 10,000 seems more likely than 12,86%.

That's why a good lawyer that wishes to raise doubt in a jury won't say that *the chance of a false positive is 0,1%*. 1 in a 10,000 seems more likely.

## We are not very good at investing either

The [Disposition Effect](https://en.wikipedia.org/wiki/Disposition_effect) is a tendency to sell assets that have increased in value, while keeping assets that have dropped in value. The [Sunk Cost Fallacy](https://en.wikipedia.org/wiki/Sunk_cost) is when we keep investing on something (that may not be that good) as a result of a previous investment. That may explain why people remain on terrible jobs or marriages.

# regret and normality

Regret is a counter-factual emotion that is evoked by the availability of reality alternatives. An abnormal event draws attention, and also evokes the idea of the event that would've been normal under the same circunstances.

> Mr. Brown never gives rides. Yesterday, he gave a ride to a guy and was mugged. Mr. Smith always gives rides. Yesterday, he gave a ride to a guy and was mugged. Who feels the most regret? Mr. Brown or Mr. Smith?

We also expect to have stronger emotional reactions (including regret) facing a result generated by a direct action.
Everything relates to the default option: keeping a stock is the default option in the stock market, so selling it is a deviation, and therefore, a natural candidate to regret.

The intense aversion to trading a risk for some other advantage is even a big part of our legal world, e.g. [the precautionary principle](https://en.wikipedia.org/wiki/Precautionary_principle).

## We don't remember things right

experiencing self -> is it hurting now?
remembering self -> how was it overall?

We confuse our past experiences with the memory we have of them. What we learn from the past is how to maximize the quality of our future memories, not necessarily our futures experiences.

*cold-hand-experiment: https://mindhacks.com/2013/05/28/why-you-might-prefer-more-pain/*

Our memories are highly influenced by two rules:
- __peak-end rule__: the global retrospective classification is the mean of the worst moment of the experience and on its end.
- __duration neglect__: the duration of an episode has no effect on its evaluation.

There's an interesting inconsistency here: we want our pleasures to be sustained and our pains to be brief, but our memory evoluted to represent the most intense moment and the end of an pleasure/pain episode.
