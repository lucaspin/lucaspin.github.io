---
layout: post
title: "Thinking, Fast and Slow"
categories: [Book, Psychology]
---

> Our recomforting conviction that the world makes sense lies on a secure foundation: our nearly unlimited capacity of ignoring our own ignorance.

We enjoy thinking of ourselves as highly rational beings, making perfect and well-balanced decisions based on nothing more than evidence and probability. Well, reading this book might show you how flawed that perception is, while offering a comprehensive insight into the shortcomings of our minds.

Warning: [Daniel Kahneman](https://pt.wikipedia.org/wiki/Daniel_Kahneman){:target="_blank"} will attempt to trick you, constantly. And then he'll offer you an explanation of why you were just tricked.

Thanks, I guess.

## WYSIATI

One recurring acronym you'll find through the whole book is **WYSIATI**. **W**hat **Y**ou **S**ee **I**s **A**ll **T**here **I**s. It is a concise way of saying that we form conclusions by only looking at the evidence we see at the moment, ignoring that we may have critical information missing, or even that the evidence we have is not trustworthy.

## Mean Regression

This was one of the chapters I enjoyed the most. He talks about [Mean Regression](https://fs.blog/2015/07/regression-to-the-mean/){:target="_blank"}, which states that in any series dependent on many variables where chance is involved, extreme outcomes tend to be followed by more moderate ones. As someone who did not know about this, several mental clicks ocurred during the reading of this chapter.

## Expert intuition

Another very controversial subject he talks about is regarding expert intuition, and how it is dangerous to trust intution when there is no regularity in the environment.

He goes through [Paul Meehl](https://en.wikipedia.org/wiki/Paul_E._Meehl){:target="_blank"}'s research and concludes that the key factors to trust an intuitive judgment are:
1. An environment regular enough to be predictable
2. The opportunity to learn those regularities through extended practice, using a quick and high quality feedback loop.

## Priming

[Priming](https://www.psychologytoday.com/intl/basics/priming){:target="_blank"} is one of the first phenomenoms he talks about.

It is weird to acknowledge the fact that we can be influenced by such simple things, like exposure to words or sounds. The simple exposure to a word causes immediate and measurable changes on how easily a lot of related words are recalled. For instance, if you see the word **EAT** before seing **SO_P**, you will probably fill the gap there with an **U** and form **SOUP**, and not **SOAP**.

That happens because of **associative activation**: ideas that were evoked trigger other related ideas, in a growing cascade of activity in our brains. It goes beyond that: the [ideomotor phenomenon](https://www.bbc.com/future/article/20130729-what-makes-the-ouija-board-move){:target="_blank"} describes how an idea can trigger an unconscious action!

## Prospect Theory

He defends his [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory){:target="_blank"} as an improvement to [Expected Utility Theory](https://en.wikipedia.org/wiki/Expected_utility_hypothesis){:target="_blank"}, by taking into account a referential point; people's current and specific situation might influence their decisions.

He extensively explains [loss aversion](https://en.wikipedia.org/wiki/Loss_aversion){:target="_blank"}, and how when directly compared or pondered, losses are taken as larger than gains.

He also describes the [Fourfold Pattern](https://www.broyhillasset.com/2012/09/21/the-fourfold-pattern/){:target="_blank"} and explains how the weights attributed to results are not identical to the probability of these results. That conclusion ends up exposing two effects:
- *Possibility Effect*: highly unlikely results receive higher weight than they deserve. The lottery ticket is the supreme example of the possibility effect.
- *Certainty Effect*: Highly certain results receive less weight than they should. We tend to exaggerate the weight of little risks and pay a higher value than expected to eliminate it completely. That's the basic principle that makes insurance policies so lucrative.

I enjoyed how he acknowledges that his theory isn't perfect, as it doesn't take into account other important aspects of a decision, like regret.

## Regret and normality

> Mr. Brown never gives rides. Yesterday, he gave a ride to a guy and was mugged. Mr. Smith always gives rides. Yesterday, he gave a ride to a guy and was mugged. Who feels the most regret? Mr. Brown or Mr. Smith?

Regret is a counter-factual emotion that is evoked by the availability of reality alternatives. An abnormal event draws attention, and also evokes the idea of the event that would've been normal under the same circunstances.

We also expect to have stronger emotional reactions (including regret) facing a result generated by a direct action.
Everything relates to the default option: keeping a stock is the default option in the stock market, so selling it is a deviation, and therefore, a natural candidate to regret.

The intense aversion to trading a risk for some other advantage is even a big part of our legal world, e.g. [the precautionary principle](https://en.wikipedia.org/wiki/Precautionary_principle){:target="_blank"}.

## Experiencing vs remembering self

We confuse our past experiences with the memory we have of them. What we learn from the past is how to maximize the quality of our future memories, not necessarily our futures experiences.

He describes an [interesting experiment](https://mindhacks.com/2013/05/28/why-you-might-prefer-more-pain/){:target="_blank"} that exposes two characteristics of how we keep memories:
- *peak-end rule*: the global retrospective classification is the mean of the worst moment of the experience and its end.
- *duration neglect*: the duration of an episode has no effect on its evaluation.

There's an interesting inconsistency here: we want our pleasures to be sustained and our pains to be brief, but our memory evoluted to represent the most intense moment and the end of a pleasure/pain episode.

## Overall

Overall, he goes through the book building a catalog of different biases/heuristics, gives them a name and describe their "symptoms". Here are some of them:
- [Availability Heuristic](https://en.wikipedia.org/wiki/Availability_heuristic){:target="_blank"}: we judge a specific topic by the first examples that comes to our mind, and by how easy they are retrieved from our memory. That way, we substitute the size of a category or the frequency of an event by the impression of how easy it is to recall some information about it, leading us to overestimate the frequency of events that capture our attention more easily. Also, personal experiences are more easily remembered than other people's experiences: couples usually overestimate their contributions to keep the house in order and think they did more than their partner.
- [Anchoring Effect](https://en.wikipedia.org/wiki/Anchoring_(cognitive_bias)){:target="_blank"}: it happens when we consider a particular value for an unknown quantity before estimating it; the result is our estimation will likely be near the considered value.
- [Halo Effect](https://en.wikipedia.org/wiki/Halo_effect){:target="_blank"}: the sequence in which we observe the characteristics of someone is very important to our final conclusion, and the first impressions have higher weights than the subsequent ones.
- [Affect Heuristic](https://en.wikipedia.org/wiki/Affect_heuristic){:target="_blank"}: we let our sympathies and antipathies about a topic determine our beliefs. That leads us to overestimate the benefits/advantages and understimate the costs/disadvantages of the things we like or believe in.
- [Hindsight Bias](https://en.wikipedia.org/wiki/Hindsight_bias){:target="_blank"}: our inability to reconstruct past beliefs leads us to underestimate how much we were surprised by past events, and that makes us believe we knew it all along.
- [Outcome Bias](https://en.wikipedia.org/wiki/Outcome_bias){:target="_blank"}: we usually blame people for making good decisions that turned out badly and give them little credit for successful decisions that are only obvious after they happened.
- [Base Rate Fallacy](https://thedecisionlab.com/biases/base-rate-fallacy/){:target="_blank"}: we tend to underestimate or even ignore base rates when some specific information about a case is available.
- [Conjunction Fallacy](https://en.wikipedia.org/wiki/Conjunction_fallacy){:target="_blank"}: we sometimes classify a conjunction of events as more likely than a single one of these events. Remember: when you specify an event in more detail, you can only reduce its probability.
- [Planning fallacy](https://en.wikipedia.org/wiki/Planning_fallacy){:target="_blank"}: when we have information about an individual case, we rarely feel the need to know statistics about the class to which the case belongs to, which usually leads us to be too optimistic when estimating the time needed for task. The [Pre-Mortem](https://en.wikipedia.org/wiki/Pre-mortem){:target="_blank"} method is a good way to battle that fallacy, where the team imagines a scenario where the project failed, and assuming that scenario is real, tries to come up with reasons that could've been the cause. Always good to remember that supressing doubt usually leads to overconfidence.
- [Endowment effect](https://en.wikipedia.org/wiki/Endowment_effect){:target="_blank"}: we value the goods we keep "to use" more than the goods we keep "to trade".
- [Sunk Cost Fallacy](https://en.wikipedia.org/wiki/Sunk_cost){:target="_blank"}: we keep investing on something (that may not be that good) just because we invested on it before.

I probably missed some of them, but these were the ones that most caught my attention.

The book can be quite repetitive at times (which isn't a bad thing), but it is certainly an interesting read.